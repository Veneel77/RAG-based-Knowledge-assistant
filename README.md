# RAG-based Knowledge assistant
An end-to-end Retrieval-Augmented Generation (RAG) app built with fully free & open-source tools. This project demonstrates how to build a knowledge assistant that answers questions from your uploaded documents (PDFs) using local embeddings + local LLMs â€” no paid APIs required.  ğŸ”¹ Features  ğŸ“„ PDF Upload & Processing â†’ Extract text and split into chunks.  ğŸ§  Embeddings â†’ sentence-transformers/all-MiniLM-L6-v2 (free & lightweight).  ğŸ” Vector Store â†’ FAISS for efficient similarity search.  ğŸ¤– Local LLM Options (choose based on your hardware):  GPT4All  â†’ simple CPU-based inference.  llama-cpp-python  â†’ supports LLaMA, Mistral, etc. in GGUF format.  ğŸ’¬ Conversational Retrieval â†’ Answers questions grounded in your docs.  ğŸŒ Streamlit UI â†’ Clean interface with Q&A, sources, and history.  ğŸš€ Free Deployment â†’ Host on Hugging Face Spaces  with Streamlit runtime.  ğŸ”¹ Tech Stack  Python, Streamlit  LangChain (chains, retrievers, orchestration)  Sentence-Transformers (embeddings)  FAISS (vector database)  GPT4All
