# RAG-based Knowledge assistant
An end-to-end Retrieval-Augmented Generation (RAG) app built with fully free & open-source tools. This project demonstrates how to build a knowledge assistant that answers questions from your uploaded documents (PDFs) using local embeddings + local LLMs — no paid APIs required.  🔹 Features  📄 PDF Upload & Processing → Extract text and split into chunks.  🧠 Embeddings → sentence-transformers/all-MiniLM-L6-v2 (free & lightweight).  🔍 Vector Store → FAISS for efficient similarity search.  🤖 Local LLM Options (choose based on your hardware):  GPT4All  → simple CPU-based inference.  llama-cpp-python  → supports LLaMA, Mistral, etc. in GGUF format.  💬 Conversational Retrieval → Answers questions grounded in your docs.  🌐 Streamlit UI → Clean interface with Q&A, sources, and history.  🚀 Free Deployment → Host on Hugging Face Spaces  with Streamlit runtime.  🔹 Tech Stack  Python, Streamlit  LangChain (chains, retrievers, orchestration)  Sentence-Transformers (embeddings)  FAISS (vector database)  GPT4All
